{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ambiguous Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I saw the man with the telescope.\n",
      "POS Tags: [('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('.', '.')]\n",
      "\n",
      "Original Sentence: Time flies like an arrow; fruit flies like a banana.\n",
      "POS Tags: [('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN'), (';', ':'), ('fruit', 'CC'), ('flies', 'NNS'), ('like', 'IN'), ('a', 'DT'), ('banana', 'NN'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ambiguous sentences\n",
    "sentences = [\n",
    "    \"I saw the man with the telescope.\",\n",
    "    \"Time flies like an arrow; fruit flies like a banana.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and analyze POS tags\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Analyze POS tags\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Print the original sentence and its POS tags\n",
    "    print(\"Original Sentence:\", sentence)\n",
    "    print(\"POS Tags:\", pos_tags)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Alphabetic strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for alphabetic strings: ['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text containing alphabetic strings\n",
    "sample_text = \"Hello 123 World!\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_a = r'[a-zA-Z]+'\n",
    "\n",
    "# Find all matches\n",
    "matches_a = re.findall(pattern_a, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for alphabetic strings:\", matches_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Lower case strings ending with 'e':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for strings ending with 'e': ['cake', 'made', 'love', 'care']\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing lower case strings ending with 'e'\n",
    "sample_text = \"The cake is made with love and care.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_b = r'\\b[a-z]*e\\b'\n",
    "\n",
    "# Find all matches\n",
    "matches_b = re.findall(pattern_b, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for strings ending with 'e':\", matches_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Lower case strings ending with the string 'ation':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for strings ending with 'ation': ['information']\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing lower case strings ending with 'ation'\n",
    "sample_text = \"The information provided is of great importance.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_c = r'\\b[a-z]*ation\\b'\n",
    "\n",
    "# Find all matches\n",
    "matches_c = re.findall(pattern_c, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for strings ending with 'ation':\", matches_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Whole numbers between 25 and 350, inclusive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for whole numbers between 25 and 350: []\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing numbers\n",
    "sample_text = \"The price ranges from 20 to 400, dollars.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_d = r'\\b(25|[3-9][0-9]|[1-2][0-9]{2}|3[0-4][0-9]|350)\\b'\n",
    "\n",
    "# Find all matches\n",
    "matches_d = re.findall(pattern_d, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for whole numbers between 25 and 350:\", matches_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) Percentage rates such as 2%, 2.5%, 10.15%, etc.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for percentage rates: ['2', '2.5', '10']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text containing percentage rates\n",
    "sample_text = \"The interest rate is 2%  2.5% 4 and the discount is 10%.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_e = r'\\b(\\d+(?:\\.\\d+)?)%'\n",
    "\n",
    "# Find all matches\n",
    "matches_e = re.findall(pattern_e, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for percentage rates:\", matches_e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (f) Strings with two consecutive repeated words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for strings with two consecutive repeated words: ['cat', 'jumped', 'over']\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing strings with two consecutive repeated words\n",
    "sample_text = \"The the cat cat jumped jumped over over the fence.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_f = r'\\b(\\w+)\\s+\\1\\b'\n",
    "\n",
    "# Find all matches\n",
    "matches_f = re.findall(pattern_f, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for strings with two consecutive repeated words:\", matches_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (g) Dates in the format of: January 29, 2024:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for dates in the format 'Month day, year': ['January', 'February']\n"
     ]
    }
   ],
   "source": [
    "# Sample text containing dates\n",
    "sample_text = \"The event is scheduled for January 29, 2024 and February 29, 2025.\"\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern_g = r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{1,2},\\s\\d{4}'\n",
    "\n",
    "# Find all matches\n",
    "matches_g = re.findall(pattern_g, sample_text)\n",
    "\n",
    "# Print matches\n",
    "print(\"Matches for dates in the format 'Month day, year':\", matches_g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BPE Algorithm and Minimum Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result: ['rate', 'rate', 'rate', 'rate', 'rate', 'rates', 'rates', 'rates', 'rating', 'rating']\n",
      "Tokenization for 'rating': 2\n",
      "Tokenization for 'rater': 0\n"
     ]
    }
   ],
   "source": [
    "# Given corpus\n",
    "corpus = \"rate rate rate rate rate rates rates rates rating rating\"\n",
    "\n",
    "# Tokenize the corpus using the BPE algorithm\n",
    "tokens = corpus.split()\n",
    "\n",
    "# Tokenization result\n",
    "print(\"Tokenization result:\", tokens)\n",
    "\n",
    "# How the following words will be tokenized: rating and rater\n",
    "print(\"Tokenization for 'rating':\", tokens.count('rating'))\n",
    "print(\"Tokenization for 'rater':\", tokens.count('rater'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Real and Fake News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = (r'C:\\Users\\Jamshaid\\Desktop\\news.csv')\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display some basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the dataset:\n",
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of word tokens (with stopwords): 5709309\n"
     ]
    }
   ],
   "source": [
    "# Number of word tokens in the corpus\n",
    "tokens_with_stopwords = [word_tokenize(text) for text in df['text']]\n",
    "num_tokens_with_stopwords = sum(len(tokens) for tokens in tokens_with_stopwords)\n",
    "print(\"\\nNumber of word tokens (with stopwords):\", num_tokens_with_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types (with stopwords): 112310\n"
     ]
    }
   ],
   "source": [
    "# Number of word types in the corpus\n",
    "word_types_with_stopwords = set(word for tokens in tokens_with_stopwords for word in tokens)\n",
    "num_word_types_with_stopwords = len(word_types_with_stopwords)\n",
    "print(\"Number of word types (with stopwords):\", num_word_types_with_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (with stopwords): 112310\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size of the corpus\n",
    "vocabulary_size_with_stopwords = len(word_types_with_stopwords)\n",
    "print(\"Vocabulary size (with stopwords):\", vocabulary_size_with_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of word tokens (without stopwords): 3526222\n",
      "Number of word types (without stopwords): 111884\n",
      "Vocabulary size (without stopwords): 111884\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_without_stopwords = [[word for word in tokens if word.lower() not in stop_words] for tokens in tokens_with_stopwords]\n",
    "num_tokens_without_stopwords = sum(len(tokens) for tokens in tokens_without_stopwords)\n",
    "\n",
    "print(\"\\nNumber of word tokens (without stopwords):\", num_tokens_without_stopwords)\n",
    "\n",
    "# Number of word types in the corpus (without stopwords)\n",
    "word_types_without_stopwords = set(word for tokens in tokens_without_stopwords for word in tokens)\n",
    "num_word_types_without_stopwords = len(word_types_without_stopwords)\n",
    "print(\"Number of word types (without stopwords):\", num_word_types_without_stopwords)\n",
    "\n",
    "# Vocabulary size of the corpus (without stopwords)\n",
    "vocabulary_size_without_stopwords = len(word_types_without_stopwords)\n",
    "print(\"Vocabulary size (without stopwords):\", vocabulary_size_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Stopwords and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of word tokens (without stopwords and lemmatization): 3526222\n",
      "Number of word types (without stopwords and lemmatization): 105665\n",
      "Vocabulary size (without stopwords and lemmatization): 105665\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lemmatized = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in tokens_without_stopwords]\n",
    "num_tokens_lemmatized = sum(len(tokens) for tokens in tokens_lemmatized)\n",
    "\n",
    "print(\"\\nNumber of word tokens (without stopwords and lemmatization):\", num_tokens_lemmatized)\n",
    "\n",
    "# Number of word types in the corpus (without stopwords and lemmatization)\n",
    "word_types_lemmatized = set(word for tokens in tokens_lemmatized for word in tokens)\n",
    "num_word_types_lemmatized = len(word_types_lemmatized)\n",
    "print(\"Number of word types (without stopwords and lemmatization):\", num_word_types_lemmatized)\n",
    "\n",
    "# Vocabulary size of the corpus (without stopwords and lemmatization)\n",
    "vocabulary_size_lemmatized = len(word_types_lemmatized)\n",
    "print(\"Vocabulary size (without stopwords and lemmatization):\", vocabulary_size_lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Stopwords and Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of word tokens (without stopwords and stemming): 3526222\n",
      "Number of word types (without stopwords and stemming): 73994\n",
      "Vocabulary size (without stopwords and stemming): 73994\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "tokens_stemmed = [[stemmer.stem(word) for word in tokens] for tokens in tokens_without_stopwords]\n",
    "num_tokens_stemmed = sum(len(tokens) for tokens in tokens_stemmed)\n",
    "\n",
    "print(\"\\nNumber of word tokens (without stopwords and stemming):\", num_tokens_stemmed)\n",
    "\n",
    "# Number of word types in the corpus (without stopwords and stemming)\n",
    "word_types_stemmed = set(word for tokens in tokens_stemmed for word in tokens)\n",
    "num_word_types_stemmed = len(word_types_stemmed)\n",
    "print(\"Number of word types (without stopwords and stemming):\", num_word_types_stemmed)\n",
    "\n",
    "# Vocabulary size of the corpus (without stopwords and stemming)\n",
    "vocabulary_size_stemmed = len(word_types_stemmed)\n",
    "print(\"Vocabulary size (without stopwords and stemming):\", vocabulary_size_stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion:\n",
      "Original Dataset:\n",
      "Number of word tokens: 5709309\n",
      "Number of word types: 112310\n",
      "Vocabulary size: 112310\n",
      "\n",
      "Without Stopwords:\n",
      "Number of word tokens: 3526222\n",
      "Number of word types: 111884\n",
      "Vocabulary size: 111884\n",
      "\n",
      "Without Stopwords and Lemmatization:\n",
      "Number of word tokens: 3526222\n",
      "Number of word types: 105665\n",
      "Vocabulary size: 105665\n",
      "\n",
      "Without Stopwords and Stemming:\n",
      "Number of word tokens: 3526222\n",
      "Number of word types: 73994\n",
      "Vocabulary size: 73994\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConclusion:\")\n",
    "print(\"Original Dataset:\")\n",
    "print(f\"Number of word tokens: {num_tokens_with_stopwords}\")\n",
    "print(f\"Number of word types: {num_word_types_with_stopwords}\")\n",
    "print(f\"Vocabulary size: {vocabulary_size_with_stopwords}\")\n",
    "\n",
    "print(\"\\nWithout Stopwords:\")\n",
    "print(f\"Number of word tokens: {num_tokens_without_stopwords}\")\n",
    "print(f\"Number of word types: {num_word_types_without_stopwords}\")\n",
    "print(f\"Vocabulary size: {vocabulary_size_without_stopwords}\")\n",
    "\n",
    "print(\"\\nWithout Stopwords and Lemmatization:\")\n",
    "print(f\"Number of word tokens: {num_tokens_lemmatized}\")\n",
    "print(f\"Number of word types: {num_word_types_lemmatized}\")\n",
    "print(f\"Vocabulary size: {vocabulary_size_lemmatized}\")\n",
    "\n",
    "print(\"\\nWithout Stopwords and Stemming:\")\n",
    "print(f\"Number of word tokens: {num_tokens_stemmed}\")\n",
    "print(f\"Number of word types: {num_word_types_stemmed}\")\n",
    "print(f\"Vocabulary size: {vocabulary_size_stemmed}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
